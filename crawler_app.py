import streamlit as st
import pandas as pd
import os
import requests
import zipfile
from datetime import datetime
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO

output_dir = "outputs"
image_folder = os.path.join(output_dir, "images")
os.makedirs(image_folder, exist_ok=True)

st.title("ğŸ« ë„ë§¤ê¾¹ ìƒí’ˆ â†’ í•™êµì¥í„° ì—‘ì…€ ìë™ ë³€í™˜ê¸°")

st.markdown("""
ë„ë§¤ê¾¹ ìƒí’ˆ URLì„ ì…ë ¥í•˜ë©´:
- ìƒí’ˆ ì •ë³´ë¥¼ ì •ë°€í•˜ê²Œ ìˆ˜ì§‘í•˜ê³ 
- ëŒ€í‘œ + ìƒì„¸ ì´ë¯¸ì§€ ì €ì¥
- í•™êµì¥í„° ì–‘ì‹ ì—‘ì…€ íŒŒì¼ê³¼ ì´ë¯¸ì§€ ZIPì„ ìƒì„±í•©ë‹ˆë‹¤.
""")

urls_input = st.text_area("ğŸ”— ë„ë§¤ê¾¹ ìƒí’ˆ URL ì…ë ¥ (ì¤„ë§ˆë‹¤ í•˜ë‚˜)", height=200)
submit = st.button("ğŸš€ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ë° ë³€í™˜")

if submit:
    urls = [u.strip() for u in urls_input.strip().split('\n') if u.strip()]
    collected = []

    with st.spinner("ìƒí’ˆ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."):
        for i, url in enumerate(urls):
            try:
                res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
                soup = BeautifulSoup(res.text, "html.parser")

                # ìƒí’ˆëª…
                title_tag = soup.select_one("meta[property='og:title']")
                if title_tag and title_tag.get('content'):
                    product_name = title_tag['content'].strip()
                else:
                    title_tag = soup.find("title")
                    product_name = title_tag.text.strip() if title_tag else ""

                # ê°€ê²©
                price_tag = soup.select_one(".price-now")
                if price_tag:
                    price_text = price_tag.text.strip()
                else:
                    price_tag = soup.select_one(".product-price")
                    price_text = price_tag.text.strip() if price_tag else ""

                # ëŒ€í‘œ ì´ë¯¸ì§€
                image_tag = soup.select_one("meta[property='og:image']")
                if image_tag and image_tag.get('content'):
                    image_url = image_tag['content']
                else:
                    image_tag = soup.select_one(".product-image img")
                    image_url = image_tag['src'] if image_tag and image_tag.get('src') else ""

                # ì„¤ëª…
                desc_tag = soup.select_one(".product-detail")
                desc = desc_tag.get_text(strip=True) if desc_tag else ""

                # ì¹´í…Œê³ ë¦¬
                category_tags = soup.select(".location a")
                categories = [tag.get_text(strip=True) for tag in category_tags[1:]]
                category1 = categories[0] if len(categories) > 0 else ""
                category2 = categories[1] if len(categories) > 1 else ""
                category3 = categories[2] if len(categories) > 2 else ""

                # ìƒì„¸ ì´ë¯¸ì§€
                detail_imgs = soup.select("#tabContents img") or soup.select(".product-detail img")
                detail_image_urls = []
                for img in detail_imgs:
                    if img.has_attr('src'):
                        src = img['src']
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            src = 'https://www.domeggook.com' + src
                        detail_image_urls.append(src)

                # í•„ìˆ˜ ì •ë³´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
                if not (product_name and price_text and image_url):
                    st.warning(f"âš ï¸ í•„ìˆ˜ ì •ë³´ ëˆ„ë½ â†’ ì œì™¸ë¨: {url}")
                    continue

                safe_name = product_name.replace(" ", "_").replace("/", "_")
                image_name = f"{safe_name}_main.jpg"
                image_path = os.path.join(image_folder, image_name)

                try:
                    img_data = requests.get(image_url, timeout=5).content
                    with open(image_path, "wb") as f:
                        f.write(img_data)
                except:
                    st.warning(f"âš ï¸ ëŒ€í‘œ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {image_url}")

                detail_image_names = []
                for j, detail_url in enumerate(detail_image_urls):
                    try:
                        response = requests.get(detail_url, timeout=5)
                        if response.status_code == 200:
                            detail_name = f"{safe_name}_detail_{j+1}.jpg"
                            detail_path = os.path.join(image_folder, detail_name)
                            with open(detail_path, "wb") as f:
                                f.write(response.content)
                            detail_image_names.append(detail_name)
                    except:
                        pass

                # ë¯¸ë¦¬ë³´ê¸°
                st.subheader(f"ğŸ“¦ {product_name}")
                try:
                    image_preview = Image.open(BytesIO(img_data))
                    st.image(image_preview, caption="ëŒ€í‘œ ì´ë¯¸ì§€", width=200)
                except:
                    st.text("(ëŒ€í‘œ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨)")

                collected.append({
                    "ê²°ê³¼": category1 or "ê¸°íƒ€",
                    "ì¹´í…Œê³ ë¦¬1": category1,
                    "ì¹´í…Œê³ ë¦¬2": category2,
                    "ì¹´í…Œê³ ë¦¬3": category3,
                    "ë¬¼í’ˆëª…": product_name,
                    "ê·œê²©": "",
                    "ëª¨ë¸ëª…": "",
                    "ì œì¡°ë²ˆí˜¸": "",
                    "ì œì¡°ì‚¬": "",
                    "ì œì¡°êµ­": "",
                    "ì†Œì¬/ì¬ì§ˆ": "",
                    "ìµœì†Œì£¼ë¬¸ìˆ˜ëŸ‰": "",
                    "íŒë§¤ë‹¨ìœ„": "ê°œ",
                    "ë³´ì¦ê¸°ê°„": "",
                    "ë‚©í’ˆ ê°€ëŠ¥ê¸°í•œ": "",
                    "ë°°ì†¡ë¹„ì¢…ë¥˜": "",
                    "ë°°ì†¡ë¹„": "",
                    "íƒ€í’ˆ ë°°ì†¡ë¹„": "",
                    "ë°°ì†¡ìœ ì˜ˆ": "",
                    "ì œì£¼ë°°ì†¡ì—¬ë¶€": "",
                    "ì œì£¼ì¶”ê°€ë°°ì†¡ë¹„": "",
                    "ì œì£¼ì¶”ê°€ë°°ì†¡ì—¬ë¶€": "",
                    "ìƒí’ˆìƒì„¸ì„¤ëª…": desc,
                    "ëŒ€í‘œì´ë¯¸ì§€1": image_name,
                    "ìƒì„¸ì´ë¯¸ì§€": ";".join(detail_image_names)
                })

            except Exception as e:
                st.error(f"âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {url} â†’ {e}")

    if collected:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_path = os.path.join(output_dir, f"í•™êµì¥í„°_ìƒí’ˆë“±ë¡_{timestamp}.xlsx")
        zip_path = os.path.join(output_dir, f"images_{timestamp}.zip")

        df = pd.DataFrame(collected)
        df.to_excel(excel_path, index=False)

        with zipfile.ZipFile(zip_path, "w") as zipf:
            for file in os.listdir(image_folder):
                zipf.write(os.path.join(image_folder, file), arcname=file)

        st.success(f"âœ… ì´ {len(df)}ê°œ ìƒí’ˆ ì •ë¦¬ ì™„ë£Œ!")
        with open(excel_path, "rb") as f:
            st.download_button("ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", data=f, file_name=os.path.basename(excel_path))
        with open(zip_path, "rb") as f:
            st.download_button("ğŸ–¼ ì´ë¯¸ì§€ ZIP ë‹¤ìš´ë¡œë“œ", data=f, file_name=os.path.basename(zip_path))